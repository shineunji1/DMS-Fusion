import cv2
import dlib
import numpy as np
import threading
from imutils import face_utils
from PIL import ImageFont, ImageDraw, Image
from dao.monitoring_dao import MonitoringDAO
from fastapi.responses import StreamingResponse
import time
import pygame
import os

class MonitoringService:
    def __init__(self):
        self.running = False  # ëª¨ë‹ˆí„°ë§ ìƒíƒœ (ì¼œì§/êº¼ì§), ê¸°ë³¸ì ìœ¼ë¡œ ì‹¤í–‰ìƒíƒœ
        self.detector = dlib.get_frontal_face_detector()  # ì–¼êµ´ ê²€ì¶œê¸°
        self.status = False # ê¸°ë³¸ê°’
        self.monitoring_dao = MonitoringDAO
        self.cap = None  # OpenCV VideoCapture ê°ì²´
        self.thread = None  # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        self.predictor = dlib.shape_predictor("models/shape_predictor_68_face_landmarks.dat")  # ëœë“œë§ˆí¬
                
        # âœ… ì‹œì„  ì´íƒˆ ê°ì§€ ë³€ìˆ˜
        self.GAZE_COUNT = 0  
        self.LAST_GAZE_TIME = None  
        self.NORMAL_GAZE_TIME = time.time()  
        self.GAZE_TIME_THRESH = 2  
        self.HEAD_ANGLE_THRESH_X = 0.08  
        self.HEAD_ANGLE_THRESH_Y = 0.1  

        self.BASE_HEAD_X = None  
        self.BASE_HEAD_Y = None  
        
        # âœ… í•œê¸€ í°íŠ¸ ì„¤ì •
        font_path = "C:/Windows/Fonts/malgun.ttf"
        self.font = ImageFont.truetype(font_path, 30)
        
        # âœ… ê²½ê³ ìŒ ì„¤ì •
        pygame.mixer.init()
        self.ALERT_SOUND = "alert.mp3"
        try:
            pygame.mixer.music.load(self.ALERT_SOUND)
        except pygame.error:
            print("âš ï¸ ê²½ê³ ìŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        # âœ… ëª¨ë¸ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
        base_dir = os.path.dirname(os.path.abspath(__file__))  # í˜„ì¬ íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬
        # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        model_dir = os.path.join(base_dir, "..", "models")
        # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
        model_path = os.path.join(model_dir, "../models/shape_predictor_68_face_landmarks.dat")

        # âœ… dlib ëœë“œë§ˆí¬ ëª¨ë¸ ë¡œë“œ
        if os.path.exists(model_path):
            print(f"âœ… dlib ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
            self.predictor = dlib.shape_predictor(model_path)
        else:
            raise FileNotFoundError(f"âŒ dlib ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            
        print(f"ğŸ“‚ í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ë””ë ‰í† ë¦¬: {base_dir}")
        print(f"ğŸ“‚ ëª¨ë¸ íŒŒì¼ ì˜ˆìƒ ê²½ë¡œ: {model_path}")
        print(f"ğŸ“‚ ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(model_path)}")
            
            

    def put_text_korean(self, img, text, pos, color=(0, 0, 255)):
        """í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ í™”ë©´ì— ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜"""
        img_pil = Image.fromarray(img)
        draw = ImageDraw.Draw(img_pil)
        draw.text(pos, text, font=self.font, fill=color)
        return np.array(img_pil)
        
    def get_monitoring_status(self):
        # return self.monitoring_dao.get_status # í˜„ì¬ ìƒíƒœ ë°˜í™˜
        # not ì œê±° status ì£¼ì†Œ ë°˜í™˜ê°’ì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ë¼ì„œ ë°˜ëŒ€ê°’ì„ ë³´ë‚´ë©´ ì•ˆë¨
        return self.status
    
    def toggle_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ìƒíƒœ ON/OFF ì „í™˜ ë° OpenCV ì‹¤í–‰/ì¢…ë£Œ"""
        # ëª¨ë‹ˆí„°ë§ ì´ˆê¸°ê°’ ë³€ê²½ì— ë”°ë¥¸ "on"/"off" => True/False ë¡œ ë³€ê²½
        if self.status == False:
            self.start_monitoring()  # âœ… OpenCSV ì‹¤í–‰
            self.status = True
            print("âœ… [Monitoring] ëª¨ë‹ˆí„°ë§ ì‹œì‘!")  # âœ… ë¡œê·¸ ì¶”ê°€

        else:
            self.stop_monitoring()   # âœ… OpenCV ì¢…ë£Œ
            self.status = False
            print("â›” [Monitoring] ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ!")  # âœ… ë¡œê·¸ ì¶”ê°€


        return self.status  # âœ… ë³€ê²½ëœ ìƒíƒœ ë°˜í™˜

    def start_monitoring(self):
        """ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (OpenCV ì‹¤í–‰) """
        if not self.running:
            self.running = True
            self.cap = cv2.VideoCapture(0) # ì›¹ìº¡
            self.monitoring_dao = MonitoringDAO()
            self.thread = threading.Thread(target=self.run_monitoring, daemon=True)
            self.thread.start()

    def stop_monitoring(self):
        """ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ """
        self.running = False
        if self.cap:
            self.cap.release()
            self.cap = None

    def run_monitoring(self):
        """OpenCV ì‹¤í–‰ (ë‹¨ìˆœ ì¹´ë©”ë¼ ì˜ìƒ í‘œì‹œ)"""
        while self.running:
            if self.cap is None:
                break

            ret, frame = self.cap.read()
            if not ret:
                print("âš ï¸ [Monitoring] ì›¹ìº ì—ì„œ í”„ë ˆì„ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤!")
                break

            # í”„ë ˆì„ ê·¸ëŒ€ë¡œ í‘œì‹œ (ì£¼ì˜ ë¶„ì‚° ê°ì§€ ì—†ìŒ)
            cv2.imshow("Monitoring", frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):  # 'q'ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ
                break

        self.stop_monitoring()
        cv2.destroyAllWindows()

    # frameìƒì„± í•¨ìˆ˜ ë°”ê¿ˆ
    def generate_frames(self):
        """ì›¹ìº  í”„ë ˆì„ì„ ì§€ì†ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ì œë„ˆë ˆì´í„° (FastAPI ìŠ¤íŠ¸ë¦¬ë°ìš©)"""
        # ì¹´ë©”ë¼ê°€ ì—†ìœ¼ë©´ ì‹œì‘
        if self.cap is None or not self.cap.isOpened():
            print("ì¹´ë©”ë¼ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
            self.cap = cv2.VideoCapture(0)  # ì¸ë±ìŠ¤ 0 ì‹œë„
            
            # ì¹´ë©”ë¼ê°€ ì—¬ì „íˆ ì—´ë¦¬ì§€ ì•Šìœ¼ë©´ ì¸ë±ìŠ¤ 1 ì‹œë„
            if not self.cap.isOpened():
                print("ì¸ë±ìŠ¤ 0ì˜ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ 1ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                self.cap = cv2.VideoCapture(1)
        
        # ì¹´ë©”ë¼ê°€ ì—¬ì „íˆ ì—´ë¦¬ì§€ ì•Šìœ¼ë©´ ì˜¤ë¥˜ ì´ë¯¸ì§€ ë°˜í™˜
        if not self.cap.isOpened():
            print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            # ì˜¤ë¥˜ ì´ë¯¸ì§€ ìƒì„± (ë¹¨ê°„ìƒ‰ ë°°ê²½)
            error_img = np.zeros((480, 640, 3), dtype=np.uint8)
            error_img[:] = (0, 0, 255)  # BGR, ë¹¨ê°„ìƒ‰
            cv2.putText(error_img, "Camera Error", (150, 240), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            
            # ì˜¤ë¥˜ ì´ë¯¸ì§€ë¥¼ í•œ ë²ˆ ë°˜í™˜
            _, buffer = cv2.imencode('.jpg', error_img)
            yield (b'--frame\r\n'
                  b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
            return
            
        print("âœ… ì¹´ë©”ë¼ê°€ ì—´ë ¸ìŠµë‹ˆë‹¤. í”„ë ˆì„ ìŠ¤íŠ¸ë¦¬ë°ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        while self.running:
            try:
                success, frame = self.cap.read()
                if not success:
                    print("âš ï¸ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    time.sleep(0.1)
                    continue
                
                # í”„ë ˆì„ì„ ê·¸ëŒ€ë¡œ ì¸ì½”ë”©í•˜ì—¬ ì „ì†¡ (ì£¼ì˜ ë¶„ì‚° ê°ì§€ ì²˜ë¦¬ ì—†ìŒ)
                _, buffer = cv2.imencode(".jpg", frame)
                frame_bytes = buffer.tobytes()
                
                yield (b"--frame\r\n"
                       b"Content-Type: image/jpeg\r\n\r\n" + frame_bytes + b"\r\n")
                
                # ë„ˆë¬´ ë¹ ë¥´ê²Œ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ 
                time.sleep(0.03)
                
            except Exception as e:
                print(f"âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                break
                
            
    def detect_distraction(self, frame):
        """ì£¼ì˜ ë¶„ì‚° ê°ì§€"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.detector(gray)

        for face in faces:
            shape = self.predictor(gray, face)
            shape = face_utils.shape_to_np(shape)

            # âœ… ë¨¸ë¦¬ ê¸°ìš¸ê¸° ê³„ì‚°
            head_x = (np.mean(shape[36:42][:, 0]) - np.mean(shape[42:48][:, 0])) / (face.right() - face.left())
            head_y = (np.mean(shape[19:27][:, 1]) - np.mean(shape[0:17][:, 1])) / (face.bottom() - face.top())

            # âœ… ê¸°ì¤€ê°’ ì„¤ì • (ì´ˆê¸°í™”)
            if self.BASE_HEAD_X is None or self.BASE_HEAD_Y is None:
                self.BASE_HEAD_X = head_x
                self.BASE_HEAD_Y = head_y

            # âœ… ì‹œì„  ì´íƒˆ ê°ì§€ ë¡œì§
            delta_x = abs(head_x - self.BASE_HEAD_X)
            delta_y = abs(head_y - self.BASE_HEAD_Y)

            if delta_x > self.HEAD_ANGLE_THRESH_X or delta_y > self.HEAD_ANGLE_THRESH_Y:
                if self.LAST_GAZE_TIME is None:
                    self.LAST_GAZE_TIME = time.time()

                elapsed_time = time.time() - self.LAST_GAZE_TIME
                if elapsed_time >= self.GAZE_TIME_THRESH:
                    self.GAZE_COUNT += 1  
                    self.LAST_GAZE_TIME = None  

                    # ğŸš¨ ê²½ê³  í‘œì‹œ
                    if self.GAZE_COUNT >= 5:
                        frame = self.put_text_korean(frame, "ğŸš¨ ìš´ì „ ì§‘ì¤‘ ê²½ê³ !", (10, 150), (0, 0, 255))
                        pygame.mixer.music.play()
                    elif self.GAZE_COUNT >= 2:
                        frame = self.put_text_korean(frame, "âš ï¸ ì „ë°© ì£¼ì‹œ ì£¼ì˜!", (10, 150), (0, 255, 255))
                        pygame.mixer.music.play()
            else:
                self.LAST_GAZE_TIME = None

        return frame
    
# def detect_distraction(self, frame):
#     print("ğŸ§ [DEBUG] ì£¼ì˜ ë¶„ì‚° ê°ì§€ í•¨ìˆ˜ ì‹¤í–‰ë¨")  # âœ… ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
#     ...
